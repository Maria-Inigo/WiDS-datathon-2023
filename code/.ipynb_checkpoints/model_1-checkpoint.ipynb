{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model using scikit learn linear regression\n",
    "This is a first attempt to train using the given data using a linear regression model\n",
    "The data will be analyzed based on startdate, locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from shapely.geometry import Point\n",
    "\n",
    "# Import additional libraries\n",
    "import os\n",
    "from IPython.display import display\n",
    "from math import radians, cos, sin, asin, sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max.columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the filepath\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + '/data'\n",
    "\n",
    "training_csv = data_dir + '/train_data.csv'\n",
    "test_csv = data_dir + '/test_data.csv'\n",
    "\n",
    "print(training_csv)\n",
    "print(test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataframes and add necessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the training data set\n",
    "training_data = pd.read_csv(training_csv)\n",
    "\n",
    "# Load the test data set\n",
    "test_data = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all column names in file\n",
    "with open('training_columns.txt', 'w', encoding='utf-8') as f:\n",
    "    for col in training_data.columns:\n",
    "        f.write(col+'\\n')\n",
    "\n",
    "with open('test_columns.txt', 'w', encoding='utf-8') as f:\n",
    "    for col in test_data.columns:\n",
    "        f.write(col+'\\n')\n",
    "\n",
    "with open('training_data_info.txt', 'w', encoding='utf-8') as f:\n",
    "    training_data.info(verbose=True, buf=f)\n",
    "\n",
    "# display(training_data.describe())\n",
    "# display(test_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the target column\n",
    "target_column = training_data.columns.difference(test_data.columns)[0]\n",
    "print(f'The target column for prediction is {target_column}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find any column with empty/null values\n",
    "print(f'Columns with null vaules in Training data are {training_data.columns[training_data.isnull().any()]}')\n",
    "print(f'Columns with null vaules in Test data are {test_data.columns[test_data.isnull().any()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Typecaste startdate to datetime for easier handling\n",
    "training_data.startdate = pd.to_datetime(training_data.startdate, format='%m/%d/%y').strftime('%s')\n",
    "test_data.startdate = pd.to_datetime(test_data.startdate, format='%m/%d/%y').strftime('%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get current precision of latitude and longitude\n",
    "loc_data = training_data[['lat','lon']]\n",
    "precision = loc_data.applymap(lambda x: len(str(x).split('.')[1]))\n",
    "\n",
    "print(f'Current precision of latitude in training data is {precision.lat.max()}')\n",
    "print(f'Current precision of longitude in training data is {precision.lon.max()}')\n",
    "\n",
    "loc_data = test_data[['lat','lon']]\n",
    "precision = loc_data.applymap(lambda x: len(str(x).split('.')[1]))\n",
    "\n",
    "print(f'Current precision of latitude in test data is {precision.lat.max()}')\n",
    "print(f'Current precision of longitude in test data is {precision.lon.max()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add standard 15 digit decimal places precision to latitude and longitude\n",
    "training_data['lat'] = training_data['lat'].round(15)\n",
    "training_data['lon'] = training_data['lon'].round(15)\n",
    "test_data['lat'] = test_data['lat'].round(15)\n",
    "test_data['lon'] = test_data['lon'].round(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to combine the latitude and longitude for easier data handling\n",
    "# 'Single-point' Haversine: Calculates the great circle distance between a point on Earth and the (0, 0) lat-long coordinate\n",
    "\n",
    "def single_pt_haversine(lat, lon, degrees=True):\n",
    "    \n",
    "    r = 6371 # Earth's radius (km)\n",
    "\n",
    "    # Convert decimal degrees to radians\n",
    "    if degrees:\n",
    "        lat, lon = map(radians, [lat, lon])\n",
    "\n",
    "    # 'Single-point' Haversine formula\n",
    "    a = sin(lat/2)**2 + cos(lat) * sin(lon/2)**2\n",
    "    d = 2 * r * asin(sqrt(a)) \n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine latitude and longitude to generate unique geolocations\n",
    "# training_data['location'] = training_data.apply(lambda x: Point(x['lon'], x['lat']), axis=1)\n",
    "# training_data['location']= training_data[['lat','lon']].values.tolist()\n",
    "\n",
    "training_data['haversine_distance'] = [single_pt_haversine(x, y) for x, y in zip(training_data.lat, training_data.lon)]\n",
    "print(f'There are {training_data.haversine_distance.nunique()} unique locations in training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data['haversine_distance'] = [single_pt_haversine(x, y) for x, y in zip(test_data.lat, test_data.lon)]\n",
    "print(f'There are {test_data.haversine_distance.nunique()} unique locations in test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get unique locations by combining test and training data and grouping by latitude and longitude\n",
    "combined_data = pd.concat([training_data,test_data], axis=0)\n",
    "combined_data['haversine_distance'] = [single_pt_haversine(x, y) for x, y in zip(combined_data.lat, combined_data.lon)]\n",
    "print(f'There are {combined_data.haversine_distance.nunique()} unique locations in combined data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split combined data into training and test dataframe with location\n",
    "training_data = combined_data.iloc[:len(training_data)]\n",
    "test_data = combined_data.iloc[len(training_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create target data\n",
    "target_data = training_data[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data wrangling starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taking a backup of the original training dataframe\n",
    "training_data_full = training_data\n",
    "training_data_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Listing the important columns\n",
    "column_names = ['haversine_distance', 'startdate', 'lat', 'lon', 'climateregions__climateregion', 'elevation__elevation', 'contest-wind-h500-14d__wind-hgt-500', 'contest-slp-14d__slp', 'contest-pres-sfc-gauss-14d__pres', 'contest-pevpr-sfc-gauss-14d__pevpr', 'contest-precip-14d__precip', 'contest-prwtr-eatm-14d__prwtr', 'contest-rhum-sig995-14d__rhum', 'contest-wind-uwnd-250-14d__wind-uwnd-250', 'contest-wind-uwnd-925-14d__wind-uwnd-925', 'contest-wind-vwnd-250-14d__wind-vwnd-250', 'contest-wind-vwnd-925-14d__wind-vwnd-925', 'mei__mei', 'mei__meirank', 'mei__nip', 'mjo1d__amplitude', 'mjo1d__phase', 'sst-2010-1', 'sst-2010-2', 'sst-2010-3', 'sst-2010-4', 'sst-2010-5', 'sst-2010-6', 'sst-2010-7', 'sst-2010-8', 'sst-2010-9', 'sst-2010-10','contest-tmp2m-14d__tmp2m']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "temp_data = training_data.drop(training_data.columns.difference(column_names),axis= 1)\n",
    "training_data = temp_data.loc[:,column_names]\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data['climateregions__climateregion']= training_data['climateregions__climateregion'].astype(str)\n",
    "training_data['startdate']= training_data['startdate'].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy data training codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = LinearRegression().fit(training_data, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the target values for the test data\n",
    "predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the mean squared error between the predicted and actual target values\n",
    "mse = mean_squared_error(target_data, predictions)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
