{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce1d0a5",
   "metadata": {},
   "source": [
    "#### CRISP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19e83e3a",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "- There are 2 datasets `train_data.csv` and `test_data.csv`\n",
    "- The `contest-tmp2m-14d__tmp2m` is the mean `(tmax+tmin / 2)` temperature and is to be predicted for the test data\n",
    "- Latitude and longitude are anonymized so latitude information cannot be used for temperature prediction\n",
    "- `startdate` indicates the start of a 14 day period\n",
    "- The data provided is between **2014** and **2016**, therefore the affect of **El Nino** is to be considered\n",
    "- `nmme` forecast values and other forecast values will not be part of the feature set used for this model\n",
    "- The 2010 data for geopotential, wind, etc. will also be discarded for this model\n",
    "- The 2010 data for sea surface temperature will be however used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "581d86dd",
   "metadata": {},
   "source": [
    "NOTE: *There are inferences below some of the data analysis/visualization which dictates the next set of data transformations*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "990ed2b4",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a05a6a2",
   "metadata": {},
   "source": [
    "#### Import libraries and set Universal params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main libraries for data analysis and modelling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# Import additional helper libraries\n",
    "import os\n",
    "import datetime as dt\n",
    "from IPython.display import display\n",
    "# import math\n",
    "# from math import radians, cos, sin, asin, sqrt\n",
    "# import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max.columns\", None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7d3a477",
   "metadata": {},
   "source": [
    "#### Set paths and create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filepath\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + '/data/'\n",
    "\n",
    "train_csv = data_dir + 'train_data.csv'\n",
    "test_csv = data_dir + 'test_data.csv'\n",
    "\n",
    "print(train_csv)\n",
    "print(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c58395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data set\n",
    "train_df_raw = pd.read_csv(train_csv)\n",
    "\n",
    "# Load the test data set\n",
    "test_df_raw = pd.read_csv(test_csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0580f858",
   "metadata": {},
   "source": [
    "#### Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display primary observations\n",
    "display(train_df_raw.info())\n",
    "display(train_df_raw.head())\n",
    "display(train_df_raw.tail())\n",
    "display(train_df_raw.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad79c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_columns.txt', 'w', encoding='utf-8') as f:\n",
    "    for col in train_df_raw.columns:\n",
    "        f.write(f'{col},{train_df_raw.dtypes[col]},{len(train_df_raw[col].unique())}\\n')\n",
    "\n",
    "with open('train_df_info.txt', 'w', encoding='utf-8') as f:\n",
    "    train_df_raw.info(verbose=True, buf=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8862b839",
   "metadata": {},
   "source": [
    "- `startdate` is an object and needs to be converted to `datetime` and later to `ordinal` Int type for usability\n",
    "- `climateregions__climateregion` is an object and needs to be converted to string type for usability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa78a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find any column with empty/null values\n",
    "print(f'Columns with null vaules in Training data are {train_df_raw.columns[train_df_raw.isnull().any()]}')\n",
    "\n",
    "# Find the target column\n",
    "target_column = train_df_raw.columns.difference(test_df_raw.columns)[0]\n",
    "print(f'The target column for prediction is {target_column}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24511eca",
   "metadata": {},
   "source": [
    "The features having null value are all prediction data and hence could be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique locations\n",
    "print('Unique locations in train data ',train_df_raw.groupby(['lat','lon']).ngroup().nunique())\n",
    "print('Unique locations in test data ',test_df_raw.groupby(['lat','lon']).ngroup().nunique()) \n",
    "print('Unique locations in combined data ',pd.concat([train_df_raw,test_df_raw], axis=0).groupby(['lat','lon']).ngroup().nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2793b218",
   "metadata": {},
   "source": [
    "Combined dataframe gives more unique locations than either train or test data. Check precision of location data to determine practicality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72fc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current precision of latitude and longitude\n",
    "precision = train_df_raw[['lat','lon']].applymap(lambda x: len(str(x).split('.')[1]))\n",
    "\n",
    "print(f'Current precision of latitude in training data is {precision.lat.max()}')\n",
    "print(f'Current precision of longitude in training data is {precision.lon.max()}')\n",
    "\n",
    "precision = test_df_raw[['lat','lon']].applymap(lambda x: len(str(x).split('.')[1]))\n",
    "\n",
    "print(f'Current precision of latitude in test data is {precision.lat.max()}')\n",
    "print(f'Current precision of longitude in test data is {precision.lon.max()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60265a09",
   "metadata": {},
   "source": [
    "- Precision 16 is too high for practical purpose. This indicates a computer or calculator was used and that no attention was paid to the fact that the extra decimals are useless.\n",
    "- The ninth decimal place is worth up to 110 microns. So, this is getting into the range of microscopy. \n",
    "- For almost any conceivable application with earth positions, this is overkill and will be more precise than the accuracy of any surveying device.\n",
    "- Decision is to reduce precision to 6 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c35417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will simply check whether a column is sorted. This is done as is_monotonic is deprecated\n",
    "def check_sort(df,col):\n",
    "    if df[col].is_monotonic_increasing or df[col].is_monotonic_decreasing:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35638aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current sorting order on important columns\n",
    "print('Sorted by index = ', check_sort(train_df_raw,'index'))\n",
    "print('Sorted by latitude = ', check_sort(train_df_raw,'lat'))\n",
    "print('Sorted by longitude = ', check_sort(train_df_raw,'lon'))\n",
    "print('Sorted by date = ', check_sort(train_df_raw,'startdate'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4062b36a",
   "metadata": {},
   "source": [
    "A useful sorting would be to sort by location (combined latitude and longitude) and then by date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "227691d3",
   "metadata": {},
   "source": [
    "##### Raw Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd894309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check initial locations on a map to understand the geography of train data\n",
    "location_df = train_df_raw[['lat','lon']].drop_duplicates().copy()\n",
    "location_df.head()\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(location_df['lat'], location_df['lon'])]\n",
    "gdf = GeoDataFrame(location_df, geometry=geometry)   \n",
    "\n",
    "#this is a simple map that goes with geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf.plot(ax=world.plot(figsize=(10, 6)), marker='o', color='red', markersize=1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check initial locations on a map to understand the geography of test data\n",
    "location_df = test_df_raw[['lat','lon']].drop_duplicates().copy()\n",
    "location_df.head()\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(location_df['lat'], location_df['lon'])]\n",
    "gdf = GeoDataFrame(location_df, geometry=geometry)   \n",
    "\n",
    "#this is a simple map that goes with geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf.plot(ax=world.plot(figsize=(10, 6)), marker='o', color='red', markersize=1);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6c6339f",
   "metadata": {},
   "source": [
    "- Both train and test data are supposedly at the same location.\n",
    "- Location is anonymized therefore it could be either omitted or converted to classification feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a65661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the spread of time in train data\n",
    "time_df = pd.DataFrame()\n",
    "time_df['startdate'] = pd.to_datetime(train_df_raw['startdate'], format='%m/%d/%y')\n",
    "time_df.groupby([time_df['startdate'].dt.year, time_df['startdate'].dt.month]).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01199028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the spread of time in test data\n",
    "time_df = pd.DataFrame()\n",
    "time_df['startdate'] = pd.to_datetime(test_df_raw['startdate'], format='%m/%d/%y')\n",
    "time_df.groupby([time_df['startdate'].dt.year, time_df['startdate'].dt.month]).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619166d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature \n",
    "temp_df = train_df_raw[['lat','lon','startdate','contest-tmp2m-14d__tmp2m']].copy()\n",
    "temp_df['startdate'] = pd.to_datetime(temp_df['startdate'], format='%m/%d/%y')\n",
    "temp_df['location'] = [Point(xy) for xy in zip(temp_df['lat'], temp_df['lon'])] \n",
    "temp_df['location_str'] = temp_df['location'].apply(lambda x: wkt.dumps(x)) # Testing Point geometry to String for use in ML training\n",
    "\n",
    "temp_df = temp_df.pivot(index='startdate', columns='location_str', values='contest-tmp2m-14d__tmp2m')\n",
    "temp_df.head()\n",
    "temp_df.plot(legend=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "487a0c89",
   "metadata": {},
   "source": [
    "- The temperature spread is even and reasoanable across the year matching the northern hemisphere seasonal temperature variation\n",
    "- The temperature spread by location indicates the locations to be spread across a large geographical area and multiple climatic regions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e983abe4",
   "metadata": {},
   "source": [
    "##### Initial Data Transformations\n",
    "Needed for the data to be properly visualized. E.g. Convert startdate from mm/dd/yy to ISO format; Sorting by location and date; Combine the latitude and longitude to create location; Reduce the precision of latitude and longitude to 6 to omit superfluous locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc822ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary functions to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e91b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new copies of the dataframes for further operations\n",
    "train_df = train_df_raw.copy()\n",
    "test_df = test_df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['startdate'] = pd.to_datetime(train_df['startdate'], format='%m/%d/%y')\n",
    "train_df['startdate_ordinal'] = train_df['startdate'].apply(lambda x:x.toordinal())\n",
    "\n",
    "test_df['startdate'] = pd.to_datetime(test_df['startdate'], format='%m/%d/%y')\n",
    "test_df['startdate_ordinal'] = test_df['startdate'].apply(lambda x:x.toordinal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18544c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to 6 decimal places precision to latitude and longitude for all practical purpose\n",
    "scale = 6\n",
    "train_df['lat'] = train_df['lat'].round(scale)\n",
    "train_df['lon'] = train_df['lon'].round(scale)\n",
    "test_df['lat'] = test_df['lat'].round(scale)\n",
    "test_df['lon'] = test_df['lon'].round(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66190de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique locations in train data ',train_df.groupby(['lat','lon']).ngroup().nunique())\n",
    "print('Unique locations in test data ',test_df.groupby(['lat','lon']).ngroup().nunique()) \n",
    "print('Unique locations in combined data ',pd.concat([train_df,test_df], axis=0).groupby(['lat','lon']).ngroup().nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba22b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now Haversine distance will not be used, instead Point geometry data for location will be converted to string for use as classification feature\n",
    "\n",
    "# Need to combine the latitude and longitude for easier data handling\n",
    "# 'Single-point' Haversine: Calculates the great circle distance between a point on Earth and the (0, 0) lat-long coordinate\n",
    "\n",
    "# def single_pt_haversine(lat, lon, degrees=True):\n",
    "    \n",
    "#     r = 6371 # Earth's radius (km)\n",
    "\n",
    "#     # Convert decimal degrees to radians\n",
    "#     if degrees:\n",
    "#         lat, lon = map(radians, [lat, lon])\n",
    "\n",
    "#     # 'Single-point' Haversine formula\n",
    "#     a = sin(lat/2)**2 + cos(lat) * sin(lon/2)**2\n",
    "#     d = 2 * r * asin(sqrt(a)) \n",
    "\n",
    "#     return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine latitude and longitude to generate unique geolocations. Convert to String for later use\n",
    "\n",
    "# train_df['haversine_distance'] = [single_pt_haversine(x, y) for x, y in zip(train_df.lat, train_df.lon)]\n",
    "train_df['location'] = [Point(xy) for xy in zip(train_df['lat'], train_df['lon'])] \n",
    "train_df['location'] = train_df['location'].apply(lambda x: wkt.dumps(x))\n",
    "\n",
    "test_df['location'] = [Point(xy) for xy in zip(test_df['lat'], test_df['lon'])] \n",
    "test_df['location'] = test_df['location'].apply(lambda x: wkt.dumps(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733737d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data is sorted by new location information\n",
    "print('Sorted by Location = ', check_sort(train_df,'location'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ada24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Climate regions to string\n",
    "train_df['climateregions__climateregion'] = train_df['climateregions__climateregion'].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1369a2f",
   "metadata": {},
   "source": [
    "#### Further analyse and Visualize Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e984f72d",
   "metadata": {},
   "source": [
    "##### Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299fd146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm if elevation is consistant for a location\n",
    "# elevation_df = train_df[['location','elevation__elevation']].drop_duplicates().copy()\n",
    "print('Unique combination of location and elevation are ',train_df[['location','elevation__elevation']].drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e716097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature against elevation\n",
    "elevation_df = train_df[['elevation__elevation','startdate','contest-tmp2m-14d__tmp2m']].drop_duplicates().copy()\n",
    "\n",
    "elevations = elevation_df.elevation__elevation.unique()\n",
    "elevations.sort()\n",
    "\n",
    "# Plot temperature against elevation for a group of elevations \n",
    "# for e in itertools.islice(elevations, 40, 43):\n",
    "#     elevation_df_1 = elevation_df[elevation_df['elevation__elevation']==e]\n",
    "#     print('Elevation ',e)\n",
    "#     elevation_df_1.plot.line(x='startdate',y='contest-tmp2m-14d__tmp2m')\n",
    "\n",
    "# Plot temperature against elevation for a range of elevations\n",
    "for e in list(filter(lambda e: (e>=100 and e<=300), elevations)):\n",
    "    elevation_df_1 = elevation_df[elevation_df['elevation__elevation'] == e]\n",
    "    title_str = 'Elevation' + str(e)\n",
    "    elevation_df_1.plot.line(x='startdate',y='contest-tmp2m-14d__tmp2m',title=title_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70f1153e",
   "metadata": {},
   "source": [
    "The effect of temperature with changing elevation is clear. Therefore this is an important feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7062343c",
   "metadata": {},
   "source": [
    "##### Climate Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7cb41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot effect of climate region on temperature\n",
    "climate_df = train_df[['climateregions__climateregion','startdate','contest-tmp2m-14d__tmp2m']].drop_duplicates().copy()\n",
    "\n",
    "climates = climate_df.climateregions__climateregion.unique()\n",
    "display(print('Unique climate regions ',len(climates)))\n",
    "\n",
    "for e in climates:\n",
    "    climate_df_1 = climate_df[climate_df['climateregions__climateregion'] == e]\n",
    "    title_str = 'Climate region ' + e\n",
    "    climate_df_1.plot.line(x='startdate',y='contest-tmp2m-14d__tmp2m',title=title_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined plot using plotly express\n",
    "\n",
    "fig = px.line(climate_df, x='startdate', \n",
    "              y='contest-tmp2m-14d__tmp2m', \n",
    "              color = 'climateregions__climateregion', \n",
    "              facet_row='climateregions__climateregion',facet_row_spacing=0.04,\n",
    "              labels={\"contest-tmp2m-14d__tmp2m\":\"Temp\", \"climateregions__climateregion\":\"Climate Region\"},\n",
    "              template = 'plotly_white', height=2000)\n",
    "\n",
    "fig.update_layout(title='Mean temperature variations by climate regions', xaxis_title='Date')\n",
    "fig.update_yaxes(visible=True, matches=None)\n",
    "fig.update_layout(annotations=[], overwrite=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42070cf7",
   "metadata": {},
   "source": [
    "The effect of climate region on temperature is evident from the plots. Therefore this is a very important feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e63f7f0c",
   "metadata": {},
   "source": [
    "#####  Multivariate ENSO index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45719675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of El Niño on temperature\n",
    "print('Unique combination of location and NIP are ',train_df[['location','mei__nip']].drop_duplicates().shape[0])\n",
    "print('Unique combination of location and NIP are ',train_df[['location','mei__mei']].drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature against MEI\n",
    "mei_df = train_df[['mei__mei','startdate','contest-tmp2m-14d__tmp2m']].drop_duplicates().copy()\n",
    "\n",
    "meis = mei_df.mei__mei.unique()\n",
    "meis.sort()\n",
    "\n",
    "# Plot temperature against elevation for a range of MEI\n",
    "for e in list(filter(lambda e: (e>=0 and e<=0.5), meis)):\n",
    "    mei_df_1 = mei_df[mei_df['mei__mei'] == e]\n",
    "    title_str = 'MEI ' + str(e)\n",
    "    mei_df_1.plot.line(x='startdate',y='contest-tmp2m-14d__tmp2m',title=title_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f48bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature against Nino Index Phase\n",
    "nip_df = train_df[['mei__nip','startdate','contest-tmp2m-14d__tmp2m']].copy()\n",
    "\n",
    "fig = px.line(nip_df, x='startdate', \n",
    "              y='contest-tmp2m-14d__tmp2m', \n",
    "              color = 'mei__nip', \n",
    "              facet_row='mei__nip',facet_row_spacing=0.04,\n",
    "              labels={\"contest-tmp2m-14d__tmp2m\":\"Temp\", \"mei__nip\":\"NIP\"},\n",
    "              template = 'plotly_white', height=300)\n",
    "\n",
    "fig.update_layout(title='Mean temperature variations by NIP', xaxis_title='Date')\n",
    "fig.update_yaxes(visible=True, matches=None)\n",
    "fig.update_layout(annotations=[], overwrite=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "074b19d9",
   "metadata": {},
   "source": [
    "Effect of MEI and NIP is not clearly visible, although slight increase in temperature is evident with nigher NIP. This is a secondary feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76089083",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0400ba89",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ce6e2",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52b99c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
